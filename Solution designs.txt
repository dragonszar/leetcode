---------------------
Question: design a request rate detector which protect a REST service service from responding to any client with 1000/sec or higher.

--
Data representations:
	Request: Pair<String, Int>, which is the clientName to TimeStamp key-value pair

--
Solution:
As first thing when RSET application retrieve a request (curReq) from web server (and before start to produce response), REST application will hash the client name to retrieve an array of time stamps (create array if not previously hashed):

HashMap<ArrayList<Int>> watching = new HashMap<ArrayList<Int>>();

Within this array, binary search for the (curReq.Value - 1000), find the index which is the youngest timestamp older than (curReq.Value - 1000), and remove all the element whose index is smaller than the found.
If the watching.get(curReq.Key).size() is begger than 1000, stop responding to this request.

--
Bad thaughts:
-
Do we need a moving window which mimic the TCP congection control buffer?
Actually no, the short answer is, there will be a buffer, but we do not care in this particular design.

The REST service definitely need a queue/buffer so that we are not missing requests, and the web server has been doing that for us; 
Or if we impletemented a web server as part of the application (like a TCP socket), and we queue the requests ourselves before generating response for it.

But we detect the request rate AFTER web server schedule a request into the REST application, which means the buffering happened before our solution comming to play.

-
Do we need a priority queue (heap) or tournament tree, so that we rank the clients and pop off the highest ranking to warn the application?
Absolutely no.

A heap or tournament tree is only needed when we want to sort (only retrieve biggest) among clients, which is a over kill with unwanted memory and computation cost.

-
Do we need to run this request rate detection logic on another thread?
No. The detection need to happen right after the REST application retrieve the request from buffer and before processing it. 
We do not want to mess up with thread synchronnization, which is complex. 
And also another CPU will not be able to accelerate the detection/processing because nothing can be paralellized here.

-
Do we need to a BST for each watching.get(curReq.Key)?
No. BST is needed when elements come out of order, and BST will have a height equal to n if the elements come in order, which is our case.

-
Do we need a counter for each watching.get(curReq.Key)?
No, ArrayList.size() is already a O(1) operation.

--
Learned:
Always question ourself if a data structure is necessary, if not, it's harming. 
To be able to do so, we need to understand what makes a data structure necessary, and keep our design target in mind.


-----------------------------
Question: Design a LRU cache, which swap/commit least used element out of cache when there's not enough space to hold a new element.

HashMap<String, Object> cache = new HashMap<String, Object>();
PriorityQueue<CacheItem> rankings = PriorityQueue<CacheItem>(); //*1

CacheItem GET(String key){	//application retrieve value with a key from the cache
	CacheItem tar;
	IF (cache.get(key) == null){
		tar = new CacheItem(key, DB.read(key));
		IF (cache.isFull() == true){
			CacheItem LRU = cache.getLRU();
			if(LRU.isDirty)
				DB.write(LRU);
			cache.remove(LRU.key);
			rankings.poll();
		}
		cache.PUT(tar);
		return tar;
	}ELSE{
		tar = cache.get(key);
	}
	
	--Refesh ranking
	tar.recent = System.now(); //*1
	rankings.remove(tar); //*2
	rankings.add(tar); //*2
	
	return tar;
}

Class CacheItem{
	String key;
	Object value;
	Boolean isDirty = false;
	TimeStamp recent;	//*1
	CacheItem(String k, Object v){key = k; value = v;}
	update(Object val){
		value = val;
		isDirty = true;
	}
	
	Boolean compare(CacheItem c1, CacheItem c2){ //*1
		if(c1.recent > c2.renct)
			return c1;
		else 
			return c2;
	}
}

As pseudo code above, we need to implement getLRU sub routine to return the LRU cache item.


-*1
The cache is already a HashMap, would need another data structure as below:
PriorityQueue<CacheItem> rankings = PriorityQueue<CacheItem>();
Where Priority queue is used so that we can retrieve the LRU in O(1).
The CacheItem need to add a member to hold the timestamp when it's used most recently used, and CacheItem need to implement the comparable interface so that the cacheitem with older timestamp wiill win.

-*2
Refesh ranking when get a cache item; Java's map and priority queue do not support updating to inserted element, thus need to remove and insert again.

-
Did we wasted memory by storing each cache item twice in both hashmap and priority queue?
No, we only new-ed CacheItem once for each, but its reference are stored twice.

-
Above is LRU cache, what about a LFU cahce?
Just need to change the priority queue's timestamp to a counter and increase the counter when refresh ranking.


---------------------------------------
Question: Event-loop vs thread-pool

How IIS/Apache working?
A server socket is listening to a port/protocol, and blocked itself by calling accept method.
When a three way hand shaking initiated by a client, a tcp connection is created and server socket unblock itself to spawn/reuse a thread and reference-into the socket returned by accept method.
The created thread will remain alive until the connection is killed. (Browser does not keep connection open unless websocket is used and web server is configued to allow it.) 
Each thread/connection is short and state-less for performance purpose.

What's the main reason the web server has been using threads, why not use process?
Thread mainly serve as a unit and boundary for web server to do life cycle management without forbidding memory sharing among threads:  upon arrivel of a request, a thread with no business object is created/reused, and it start to construct most of the business objects from  database query results (the engine class' init method in MVC), upon finishing of serving this request, web server delete all business ojects in the thread (or even delete the thread if not reused); but there can be objects alive for any time span or even alive as long as the application process running, and these objects can be shared among different threads(, but the shared memory need to be thread-safe where the complexity of multi-threading come to play.)
The thread also provides concurrency (utilize multi-core, or timely interleave the same core to higher responsiveness) with higher memory effeciency, assuming there are thread safe objects shared by multiple threads, which can be a huge library. 

What CANNOT be a excuse to use thread but not process?
Create a logical boundary as well as accessibility among objects in memory: actually it's the OOP object capsulation who's providing the logical and security boundary, even single threaded JavaScript can do that.


What is bad about web server?
A living thread consume memory; (Connections also consume a little memory but can be ignored)
Switching between thread cost CPU;
Have to do multi-threading sometimes, which requires experience and introduce complexity when maintain and testing;
Initializing connection for each request is slow (not only because the hand shake, but also because the slow-start phase of the TCP)
Web server cannot push data/event to client, has to be twisted tricks to make real-time application.  
Because one thread is required for each connection, and there cannot be too many threads, thus each connection is short (HTTP 1.0) and do not keep alive (though web server might keep connection alive as optimization when load is low).


Do we always need the boundary of memory between requests?
Actually we never need the "boundary of memory between requests", (but web server made the boundary this way with no choice, ) what we really need is a boundary between sessions ( or users at least ), the boundary between the requests of same session/user actually introduced complexity.

Can we have keep-alive connection, bi-direction communication between browser and web server? 
Yes, but we cannot use common sense web servers any more, instead, we will write socket applications:
Keep the connection alive through a session, and only create new thread for a new session, and do not kill the thread until a session is done. (For example, a session start when a single player game begin, and end on quit.) Sometime we even want to server multiple connections/sessions with one thread (say, multiple players in a volley ball game)

What is bad about this cool socket based solution?
It's even more intensive multi-threading, even more complex and hard to develop and debug then web server based applications.
We also on our won to maintain the quality of service (monitoring heavy load and load balance, etc.)

What if we do not want the complexity of socket programming, but still want the keep-alive connection, bi-direction communication between browser and server? 
The event-loop based Njinx managed single-threaded event-driven nodeJS processes serving WebSocket. 

One running NodeJs application is one single thread process, every request of different sessions of different user will be served by the same thread (no boundary in memory at all).
No complexity of multi-threading, but with some complexity of mingled objects/events belonging to different users.
Huge advantage of productivity by using JavaScript.

But what is bad about NodeJS then?
The lack of security boundary in memory. 
Cannot utilize multi-core if not multi-processing.
Cannot share memory between processes, which means harder communications and more memory consumption.
Cannot fully apply design parterns in OOP (which is not too bad).
Lower responsiveness, anything long running will cause incomming request being ignored.

Is NodeJS really that bad on all above perspectives? Maybe not:


So, nature of the application and its requirement decides the technology we have to use. 
 