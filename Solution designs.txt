---------------------
Question: design a request rate detector which protect a REST service service from responding to any client with 1000/sec or higher.

--
Data representations:
	Request: Pair<String, Int>, which is the clientName to TimeStamp key-value pair

--
Solution:
As first thing when RSET application retrieve a request (curReq) from web server (and before start to produce response), REST application will hash the client name to retrieve an array of time stamps (create array if not previously hashed):

HashMap<ArrayList<Int>> watching = new HashMap<ArrayList<Int>>();

Within this array, binary search for the (curReq.Value - 1000), find the index which is the youngest timestamp older than (curReq.Value - 1000), and remove all the element whose index is smaller than the found.
If the watching.get(curReq.Key).size() is begger than 1000, stop responding to this request.

--
Bad thaughts:
-
Do we need a moving window which mimic the TCP congection control buffer?
Actually no, the short answer is, there will be a buffer, but we do not care in this particular design.

The REST service definitely need a queue/buffer so that we are not missing requests, and the web server has been doing that for us; 
Or if we impletemented a web server as part of the application (like a TCP socket), and we queue the requests ourselves before generating response for it.

But we detect the request rate AFTER web server schedule a request into the REST application, which means the buffering happened before our solution comming to play.

-
Do we need a priority queue (heap) or tournament tree, so that we rank the clients and pop off the highest ranking to warn the application?
Absolutely no.

A heap or tournament tree is only needed when we want to sort (only retrieve biggest) among clients, which is a over kill with unwanted memory and computation cost.

-
Do we need to run this request rate detection logic on another thread?
No. The detection need to happen right after the REST application retrieve the request from buffer and before processing it. 
We do not want to mess up with thread synchronnization, which is complex. 
And also another CPU will not be able to accelerate the detection/processing because nothing can be paralellized here.

-
Do we need to a BST for each watching.get(curReq.Key)?
No. BST is needed when elements come out of order, and BST will have a height equal to n if the elements come in order, which is our case.

-
Do we need a counter for each watching.get(curReq.Key)?
No, ArrayList.size() is already a O(1) operation.

--
Learned:
Always question ourself if a data structure is necessary, if not, it's harming. 
To be able to do so, we need to understand what makes a data structure necessary, and keep our design target in mind.


-----------------------------
Question: Design a LRU cache, which swap/commit least used element out of cache when there's not enough space to hold a new element.

HashMap<String, Object> cache = new HashMap<String, Object>();
PriorityQueue<CacheItem> rankings = PriorityQueue<CacheItem>(); //*1

CacheItem GET(String key){	//application retrieve value with a key from the cache
	CacheItem tar;
	IF (cache.get(key) == null){
		tar = new CacheItem(key, DB.read(key));
		IF (cache.isFull() == true){
			CacheItem LRU = cache.getLRU();
			if(LRU.isDirty)
				DB.write(LRU);
			cache.remove(LRU.key);
			rankings.poll();
		}
		cache.PUT(tar);
		return tar;
	}ELSE{
		tar = cache.get(key);
	}
	
	--Refesh ranking
	tar.recent = System.now(); //*1
	rankings.remove(tar); //*2
	rankings.add(tar); //*2
	
	return tar;
}

Class CacheItem{
	String key;
	Object value;
	Boolean isDirty = false;
	TimeStamp recent;	//*1
	CacheItem(String k, Object v){key = k; value = v;}
	update(Object val){
		value = val;
		isDirty = true;
	}
	
	Boolean compare(CacheItem c1, CacheItem c2){ //*1
		if(c1.recent > c2.renct)
			return c1;
		else 
			return c2;
	}
}

As pseudo code above, we need to implement getLRU sub routine to return the LRU cache item.


-*1
The cache is already a HashMap, would need another data structure as below:
PriorityQueue<CacheItem> rankings = PriorityQueue<CacheItem>();
Where Priority queue is used so that we can retrieve the LRU in O(1).
The CacheItem need to add a member to hold the timestamp when it's used most recently used, and CacheItem need to implement the comparable interface so that the cacheitem with older timestamp wiill win.

-*2
Refesh ranking when get a cache item; Java's map and priority queue do not support updating to inserted element, thus need to remove and insert again.

-
Did we wasted memory by storing each cache item twice in both hashmap and priority queue?
No, we only new-ed CacheItem once for each, but its reference are stored twice.

-
Above is LRU cache, what about a LFU cahce?
Just need to change the priority queue's timestamp to a counter and increase the counter when refresh ranking.


---------------------------------------
Question: Event-loop vs thread-pool

How IIS/Apache working?
A server socket is listening to a port/protocol, and blocked itself by calling accept method.
When a three way hand shaking initiated by a client, a tcp connection is created and server socket unblock itself to spawn/reuse a thread and reference the socket returned by accept method.
The create thread will remain alive until the connection is killed. (Browser does not keep connection open unless websocket is used.) Each request from browser initialize a new connection which is killed after served by server, which is short and state-less.

What is bad about web server?
A living thread consume memory; (Connections also consume a little memory but can be ignored)
Switching between thread cost CPU;
Have to do multi-threading sometimes, which requires experience and introduce complexity when maintain and testing;
Initializing connection for each request is slow (not only because the hand shake, but also because the slow-start phase of the TCP)
Web server cannot push data/event to client, has to be twisted tricks to make real-time application.  

Why web server is doing that?
To create a boundary among objects in memory, so that when we writing logic, we mostly programming against the memories belonging to (current request of) the current user. In most scenarios it's basic requirement from security complexity as well as security.
Because one thread is required for each connection, and there cannot be too many threads, thus each connection is short (HTTP 1.0) and do not keep alive (though web server might keep connection alive as optimization when load is low).

Do we always need the boundary of memory between requests?
Actually we never need the "boundary of memory between requests", (but web server made the boundary this way with no choice, ) what we really need is a boundary between sessions ( or users at least ), the boundary between the requests of same session/user actually introduced complexity.

Can we have keep-alive connection, bi-direction communication between browser and server? 
Yes, but we cannot use common sense web servers any more, instead, we will write socket applications:
Keep the connection alive through a session, and only create new thread for a new session, and do not kill the thread until a session is done. (For example, a session start when a Pingpong game player enter a table and end when he quit) Sometime we even want to server multiple connections/sessions with one thread (say, multiple players in a volley ball game)

What is bad about this cool socket based solution?
It's even more intensive multi-threading, even more complex and hard to develop and debug then web server based applications.
We also on our won to maintain the quality of service (monitoring heavy load and load balance, etc.)

What if we do not what the complexity of socket programming, but still want the keep-alive connection, bi-direction communication between browser and server? 
The event-loop based Njinx managed single-threaded event-driven nodeJS processes serving WebSocket. 

One running NodeJs application is one single thread process, every request of different sessions of different user will be served by the same thread (no boundary in memory at all).
No complexity of multi-threading, but with some complexity of mingled objects/events belonging to different users.
Huge advantage of productivity by using JavaScript.

But what is bad about NodeJS then?
Due to the lack of "boundary in memory", we will start to do multi-process every soon (if we do not want to keep too many users/sessions/groupOfSession in mind when programming).
But multi-process consume memories by loading libraries for each process, which is previously loaded only once and shared among threads in multi-threading.

So, nature of the application and its requirement decides the technology we have to use. 
 