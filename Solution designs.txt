---------------------
Question: design a request rate detector which protect a REST service service from responding to any client with 1000/sec or higher.

--
Data representations:
	Request: Pair<String, Int>, which is the clientName to TimeStamp key-value pair

--
Solution:
As first thing when RSET application retrieve a request (curReq) from web server (and before start to produce response), REST application will hash the client name to retrieve an array of time stamps (create array if not previously hashed):

HashMap<ArrayList<Int>> watching = new HashMap<ArrayList<Int>>();

Within this array, binary search for the (curReq.Value - 1000), find the index which is the youngest timestamp older than (curReq.Value - 1000), and remove all the element whose index is smaller than the found.
If the watching.get(curReq.Key).size() is begger than 1000, stop responding to this request.

--
Bad thaughts:
-
Do we need a moving window which mimic the TCP congection control buffer?
Actually no, the short answer is, there will be a buffer, but we do not care in this particular design.

The REST service definitely need a queue/buffer so that we are not missing requests, and the web server has been doing that for us; 
Or if we impletemented a web server as part of the application (like a TCP socket), and we queue the requests ourselves before generating response for it.

But we detect the request rate AFTER web server schedule a request into the REST application, which means the buffering happened before our solution comming to play.

-
Do we need a priority queue (heap) or tournament tree, so that we rank the clients and pop off the highest ranking to warn the application?
Absolutely no.

A heap or tournament tree is only needed when we want to sort (only retrieve biggest) among clients, which is a over kill with unwanted memory and computation cost.

-
Do we need to run this request rate detection logic on another thread?
No. The detection need to happen right after the REST application retrieve the request from buffer and before processing it. 
We do not want to mess up with thread synchronnization, which is complex. 
And also another CPU will not be able to accelerate the detection/processing because nothing can be paralellized here.

-
Do we need to a BST for each watching.get(curReq.Key)?
No. BST is needed when elements come out of order, and BST will have a height equal to n if the elements come in order, which is our case.

-
Do we need a counter for each watching.get(curReq.Key)?
No, ArrayList.size() is already a O(1) operation.

--
Learned:
Always question ourself if a data structure is necessary, if not, it's harming. 
To be able to do so, we need to understand what makes a data structure necessary, and keep our design target in mind.


-----------------------------
Question: Design a LRU cache, which swap/commit least used element out of cache when there's not enough space to hold a new element.

HashMap<String, Object> cache = new HashMap<String, Object>();
PriorityQueue<CacheItem> rankings = PriorityQueue<CacheItem>(); //*1

CacheItem GET(String key){	//application retrieve value with a key from the cache
	CacheItem tar;
	IF (cache.get(key) == null){
		tar = new CacheItem(key, DB.read(key));
		IF (cache.isFull() == true){
			CacheItem LRU = cache.getLRU();
			if(LRU.isDirty)
				DB.write(LRU);
			cache.remove(LRU.key);
			rankings.poll();
		}
		cache.PUT(tar);
		return tar;
	}ELSE{
		tar = cache.get(key);
	}
	
	--Refesh ranking
	tar.recent = System.now(); //*1
	rankings.remove(tar); //*2
	rankings.add(tar); //*2
	
	return tar;
}

Class CacheItem{
	String key;
	Object value;
	Boolean isDirty = false;
	TimeStamp recent;	//*1
	CacheItem(String k, Object v){key = k; value = v;}
	update(Object val){
		value = val;
		isDirty = true;
	}
	
	Boolean compare(CacheItem c1, CacheItem c2){ //*1
		if(c1.recent > c2.renct)
			return c1;
		else 
			return c2;
	}
}

As pseudo code above, we need to implement getLRU sub routine to return the LRU cache item.


-*1
The cache is already a HashMap, would need another data structure as below:
PriorityQueue<CacheItem> rankings = PriorityQueue<CacheItem>();
Where Priority queue is used so that we can retrieve the LRU in O(1).
The CacheItem need to add a member to hold the timestamp when it's used most recently used, and CacheItem need to implement the comparable interface so that the cacheitem with older timestamp wiill win.

-*2
Refesh ranking when get a cache item; Java's map and priority queue do not support updating to inserted element, thus need to remove and insert again.

-
Did we wasted memory by storing each cache item twice in both hashmap and priority queue?
No, we only new-ed CacheItem once for each, but its reference are stored twice.

-
Above is LRU cache, what about a LFU cahce?
Just need to change the priority queue's timestamp to a counter and increase the counter when refresh ranking.